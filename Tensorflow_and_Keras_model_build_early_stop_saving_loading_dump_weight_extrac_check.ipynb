{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tensorflow and Keras_model build_early stop-saving-loading-dump-weight extrac check.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPyGTFzDtke0TEUMZGqhEi5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chandraSekar123/ML_Deployment_Django/blob/master/Tensorflow_and_Keras_model_build_early_stop_saving_loading_dump_weight_extrac_check.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0QCY-7dUEqB",
        "colab_type": "code",
        "outputId": "44da9a76-2416-48b6-d7eb-7ccba08e726d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "    COLAB = True\n",
        "    print(\"Note: using Google CoLab\")\n",
        "except:\n",
        "    print(\"Note: not using Google CoLab\")\n",
        "    COLAB = False"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Note: using Google CoLab\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q667I311UhOm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f29d144e-f4d7-4153-b81b-e5c01d1df699"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(\"Tensor Flow Version: {}\".format(tf.__version__))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor Flow Version: 2.2.0-rc3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EikXihTuVWkn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "50aeea36-d858-4a4e-ad47-833c01d6267c"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Create a Constant op that produces a 1x2 matrix.  The op is\n",
        "# added as a node to the default graph.\n",
        "#\n",
        "# The value returned by the constructor represents the output\n",
        "# of the Constant op.\n",
        "matrix1 = tf.constant([[3., 3.]])\n",
        "\n",
        "# Create another Constant that produces a 2x1 matrix.\n",
        "matrix2 = tf.constant([[2.],[2.]])\n",
        "\n",
        "# Create a Matmul op that takes 'matrix1' and 'matrix2' as inputs.\n",
        "# The returned value, 'product', represents the result of the matrix\n",
        "# multiplication.\n",
        "product = tf.matmul(matrix1, matrix2)\n",
        "\n",
        "print(\"shape of marix 1  is\",matrix2.get_shape)\n",
        "print(\"shape of marix  2 is\",matrix2.get_shape)\n",
        "print(product)\n",
        "\n",
        "print(float(product))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape of marix 1  is <bound method _EagerTensorBase.get_shape of <tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
            "array([[2.],\n",
            "       [2.]], dtype=float32)>>\n",
            "shape of marix  2 is <bound method _EagerTensorBase.get_shape of <tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
            "array([[2.],\n",
            "       [2.]], dtype=float32)>>\n",
            "tf.Tensor([[12.]], shape=(1, 1), dtype=float32)\n",
            "12.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbrbSCSMV1nB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e0009aec-4642-4d50-fcb0-3acc92cfb763"
      },
      "source": [
        "# Enter an interactive TensorFlow Session.\n",
        "import tensorflow as tf\n",
        "\n",
        "x = tf.Variable([1.0, 2.0])\n",
        "a = tf.constant([3.0, 3.0])\n",
        "\n",
        "# Add an op to subtract 'a' from 'x'.  Run it and print the result\n",
        "sub = tf.subtract(x, a)\n",
        "print(sub)\n",
        "print(sub.numpy())\n",
        "y=sub.numpy()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([-2. -1.], shape=(2,), dtype=float32)\n",
            "[-2. -1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LICBVnxaWdVC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "42759ad0-3f70-4802-81c8-e93cdec17cf2"
      },
      "source": [
        "x.assign([4.0, 6.0])\n",
        "print(sub)\n",
        "print(sub.numpy())"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([-2. -1.], shape=(2,), dtype=float32)\n",
            "[-2. -1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPgwat-uXUfz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "e0b8f9fd-fe41-4449-e73a-e250ea4f2abc"
      },
      "source": [
        "Introduction to Keras\n",
        "Keras is a layer on top of Tensorflow that makes it much easier to create neural networks. Rather than define the graphs, like you see above, you define the individual layers of the network with a much more high level API. Unless you are performing research into entirely new structures of deep neural networks it is unlikely that you need to program TensorFlow directly."
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-28-4c709d531f58>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Introduction to Keras\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1xeRknEYQyO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "import pandas as pd\n",
        "import io\n",
        "import os\n",
        "import requests\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "\n",
        "df = pd.read_csv(\n",
        "    \"https://data.heatonresearch.com/data/t81-558/auto-mpg.csv\", \n",
        "    na_values=['NA', '?'])\n",
        "\n",
        "cars = df['name']\n",
        "\n",
        "# Handle missing value\n",
        "df['horsepower'] = df['horsepower'].fillna(df['horsepower'].median())\n",
        "\n",
        "# Pandas to Numpy\n",
        "x = df[['cylinders', 'displacement', 'horsepower', 'weight',\n",
        "       'acceleration', 'year', 'origin']].values\n",
        "y = df['mpg'].values # regression\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wwR9ASNYdNY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9b104b1c-6717-4bf6-fca3-92a0ec7cfc6d"
      },
      "source": [
        "# Build the neural network\n",
        "n_features=x.shape[1]\n",
        "nhidden1=25 \n",
        "nhidden2=10\n",
        "nhidden3=1\n",
        "act1='relu'\n",
        "act2='relu'\n",
        "mymodel=Sequential()\n",
        "mymodel.add(Dense(units=nhidden1,input_dim=n_features,activation=act1))\n",
        "mymodel.add(Dense(units=nhidden2,activation=act2))\n",
        "mymodel.add(Dense(units=nhidden3))\n",
        "mymodel.compile(loss='mean_squared_error', optimizer='adam')\n",
        "mymodel.fit(x,y,verbose=1,epochs=100)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 103342.0469\n",
            "Epoch 2/100\n",
            "13/13 [==============================] - 0s 1ms/step - loss: 17396.2246\n",
            "Epoch 3/100\n",
            "13/13 [==============================] - 0s 1ms/step - loss: 773.5613\n",
            "Epoch 4/100\n",
            "13/13 [==============================] - 0s 1ms/step - loss: 374.1763\n",
            "Epoch 5/100\n",
            "13/13 [==============================] - 0s 1ms/step - loss: 381.2231\n",
            "Epoch 6/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 339.9774\n",
            "Epoch 7/100\n",
            "13/13 [==============================] - 0s 1ms/step - loss: 302.2349\n",
            "Epoch 8/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 275.1102\n",
            "Epoch 9/100\n",
            "13/13 [==============================] - 0s 1ms/step - loss: 255.5471\n",
            "Epoch 10/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 231.3329\n",
            "Epoch 11/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 219.7696\n",
            "Epoch 12/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 208.7131\n",
            "Epoch 13/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 209.3349\n",
            "Epoch 14/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 200.2492\n",
            "Epoch 15/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 199.5757\n",
            "Epoch 16/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 196.0539\n",
            "Epoch 17/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 195.0620\n",
            "Epoch 18/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 200.5633\n",
            "Epoch 19/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 196.2246\n",
            "Epoch 20/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 195.5147\n",
            "Epoch 21/100\n",
            "13/13 [==============================] - 0s 1ms/step - loss: 195.6945\n",
            "Epoch 22/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 200.3031\n",
            "Epoch 23/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 193.8085\n",
            "Epoch 24/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 197.6821\n",
            "Epoch 25/100\n",
            "13/13 [==============================] - 0s 1ms/step - loss: 193.6473\n",
            "Epoch 26/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 194.8244\n",
            "Epoch 27/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 198.2598\n",
            "Epoch 28/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 194.7487\n",
            "Epoch 29/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 197.6871\n",
            "Epoch 30/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 194.9676\n",
            "Epoch 31/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 195.0260\n",
            "Epoch 32/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 194.3621\n",
            "Epoch 33/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 191.6847\n",
            "Epoch 34/100\n",
            "13/13 [==============================] - 0s 1ms/step - loss: 193.4570\n",
            "Epoch 35/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 193.8679\n",
            "Epoch 36/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 199.0799\n",
            "Epoch 37/100\n",
            "13/13 [==============================] - 0s 1ms/step - loss: 195.5099\n",
            "Epoch 38/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 192.4076\n",
            "Epoch 39/100\n",
            "13/13 [==============================] - 0s 1ms/step - loss: 198.3069\n",
            "Epoch 40/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 192.3814\n",
            "Epoch 41/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 194.1924\n",
            "Epoch 42/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 191.7901\n",
            "Epoch 43/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 190.7084\n",
            "Epoch 44/100\n",
            "13/13 [==============================] - 0s 1ms/step - loss: 193.4066\n",
            "Epoch 45/100\n",
            "13/13 [==============================] - 0s 1ms/step - loss: 189.6610\n",
            "Epoch 46/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 189.0876\n",
            "Epoch 47/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 189.1878\n",
            "Epoch 48/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 194.6712\n",
            "Epoch 49/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 190.9714\n",
            "Epoch 50/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 190.0558\n",
            "Epoch 51/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 193.7422\n",
            "Epoch 52/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 189.8788\n",
            "Epoch 53/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 189.0650\n",
            "Epoch 54/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 192.0618\n",
            "Epoch 55/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 192.4693\n",
            "Epoch 56/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 195.8275\n",
            "Epoch 57/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 192.3754\n",
            "Epoch 58/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 191.5319\n",
            "Epoch 59/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 187.8359\n",
            "Epoch 60/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 188.8842\n",
            "Epoch 61/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 188.5956\n",
            "Epoch 62/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 188.6923\n",
            "Epoch 63/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 187.5671\n",
            "Epoch 64/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 187.9638\n",
            "Epoch 65/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 186.9895\n",
            "Epoch 66/100\n",
            "13/13 [==============================] - 0s 1ms/step - loss: 184.0405\n",
            "Epoch 67/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 186.8188\n",
            "Epoch 68/100\n",
            "13/13 [==============================] - 0s 1ms/step - loss: 185.0942\n",
            "Epoch 69/100\n",
            "13/13 [==============================] - 0s 1ms/step - loss: 189.1350\n",
            "Epoch 70/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 183.9223\n",
            "Epoch 71/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 184.3927\n",
            "Epoch 72/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 187.3927\n",
            "Epoch 73/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 184.3437\n",
            "Epoch 74/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 183.7430\n",
            "Epoch 75/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 187.3397\n",
            "Epoch 76/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 188.1272\n",
            "Epoch 77/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 180.7397\n",
            "Epoch 78/100\n",
            "13/13 [==============================] - 0s 1ms/step - loss: 183.0949\n",
            "Epoch 79/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 183.3248\n",
            "Epoch 80/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 183.9432\n",
            "Epoch 81/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 184.2894\n",
            "Epoch 82/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 185.3102\n",
            "Epoch 83/100\n",
            "13/13 [==============================] - 0s 1ms/step - loss: 180.8617\n",
            "Epoch 84/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 180.9974\n",
            "Epoch 85/100\n",
            "13/13 [==============================] - 0s 1ms/step - loss: 182.2318\n",
            "Epoch 86/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 178.7119\n",
            "Epoch 87/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 181.7184\n",
            "Epoch 88/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 180.7279\n",
            "Epoch 89/100\n",
            "13/13 [==============================] - 0s 1ms/step - loss: 179.4018\n",
            "Epoch 90/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 179.8242\n",
            "Epoch 91/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 181.8072\n",
            "Epoch 92/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 181.9585\n",
            "Epoch 93/100\n",
            "13/13 [==============================] - 0s 1ms/step - loss: 178.7284\n",
            "Epoch 94/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 179.9716\n",
            "Epoch 95/100\n",
            "13/13 [==============================] - 0s 1ms/step - loss: 178.9616\n",
            "Epoch 96/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 181.6693\n",
            "Epoch 97/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 176.7599\n",
            "Epoch 98/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 179.2304\n",
            "Epoch 99/100\n",
            "13/13 [==============================] - 0s 1ms/step - loss: 180.5288\n",
            "Epoch 100/100\n",
            "13/13 [==============================] - 0s 2ms/step - loss: 175.7121\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f35a6fa4a90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgfXdEfyZTJW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "814def73-5f5f-43a2-bd97-0756bffc4bbd"
      },
      "source": [
        "Controlling the Amount of Output\n",
        "One line is produced for each training epoch. You can eliminate this output by setting the verbose setting of the fit command:\n",
        "\n",
        "verbose=0 - No progress output (use with Juputer if you do not want output)\n",
        "verbose=1 - Display progress bar, does not work well with Jupyter\n",
        "verbose=2 - Summary progress output (use with Jupyter if you want to know the loss at each epoch)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-47-12a1e5ee7e46>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Controlling the Amount of Output\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H50GMcnCZU1e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "ebda4a39-b3f8-4dc6-f869-4ffbb8c3e5c5"
      },
      "source": [
        "pred = mymodel.predict(x)\n",
        "print(f\"Shape: {pred.shape}\")\n",
        "print(pred[0:10])"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape: (398, 1)\n",
            "[[23.707472]\n",
            " [25.076267]\n",
            " [23.31501 ]\n",
            " [23.378265]\n",
            " [23.417955]\n",
            " [29.476723]\n",
            " [29.594769]\n",
            " [29.335264]\n",
            " [30.136753]\n",
            " [26.145416]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "km36TrF7bMZa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "c7cc3db8-cf82-4a34-d317-a3e85db07fc2"
      },
      "source": [
        "We would like to see how good these predictions are. We know what the correct MPG is for each car, so we can measure how close the neural network was."
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-54-c8f1d2b38b27>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    We would like to see how good these predictions are. We know what the correct MPG is for each car, so we can measure how close the neural network was.\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtNLOO2lcHQZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b9ea5d15-22f5-47ac-f401-8e130f6a549d"
      },
      "source": [
        "# Measure RMSE error.  RMSE is common for regression.\n",
        "score = np.sqrt(metrics.mean_squared_error(pred,y))\n",
        "print(f\"Final score (RMSE): {score}\")"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final score (RMSE): 13.358812495721514\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fsz_3K_ocJCR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "13cfb9a3-22de-4eac-9814-540383105c7e"
      },
      "source": [
        "# Sample predictions\n",
        "for i in range(10):\n",
        "    print(f\"{i+1}. Car name: {cars[i]}, MPG: {y[i]}, \" \n",
        "          + f\"predicted MPG: {pred[i]}\")"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1. Car name: chevrolet chevelle malibu, MPG: [18.], predicted MPG: [23.707472]\n",
            "2. Car name: buick skylark 320, MPG: [15.], predicted MPG: [25.076267]\n",
            "3. Car name: plymouth satellite, MPG: [18.], predicted MPG: [23.31501]\n",
            "4. Car name: amc rebel sst, MPG: [16.], predicted MPG: [23.378265]\n",
            "5. Car name: ford torino, MPG: [17.], predicted MPG: [23.417955]\n",
            "6. Car name: ford galaxie 500, MPG: [15.], predicted MPG: [29.476723]\n",
            "7. Car name: chevrolet impala, MPG: [14.], predicted MPG: [29.594769]\n",
            "8. Car name: plymouth fury iii, MPG: [14.], predicted MPG: [29.335264]\n",
            "9. Car name: pontiac catalina, MPG: [14.], predicted MPG: [30.136753]\n",
            "10. Car name: amc ambassador dpl, MPG: [15.], predicted MPG: [26.145416]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vggbTx2OcuaC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Simple TensorFlow Classification: Iris\n",
        "\n",
        "import pandas as pd\n",
        "import io\n",
        "import requests\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "df = pd.read_csv(\n",
        "    \"https://data.heatonresearch.com/data/t81-558/iris.csv\", \n",
        "    na_values=['NA', '?'])\n",
        "\n",
        "# Convert to numpy - Classification\n",
        "x = df[['sepal_l', 'sepal_w', 'petal_l', 'petal_w']].values\n",
        "dummies = pd.get_dummies(df['species']) # Classification\n",
        "species = dummies.columns\n",
        "y = dummies.values\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80kiFfaYdQI8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d9835805-e5d4-4bf8-9977-c7b1d7a24ba5"
      },
      "source": [
        "# Build neural network\n",
        "model = Sequential()\n",
        "model.add(Dense(50, input_dim=x.shape[1], activation='relu')) # Hidden 1\n",
        "model.add(Dense(25, activation='relu')) # Hidden 2\n",
        "model.add(Dense(y.shape[1],activation='softmax')) # Output\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "model.fit(x,y,verbose=2,epochs=100)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "5/5 - 0s - loss: 1.1652\n",
            "Epoch 2/100\n",
            "5/5 - 0s - loss: 0.9519\n",
            "Epoch 3/100\n",
            "5/5 - 0s - loss: 0.8911\n",
            "Epoch 4/100\n",
            "5/5 - 0s - loss: 0.8576\n",
            "Epoch 5/100\n",
            "5/5 - 0s - loss: 0.8267\n",
            "Epoch 6/100\n",
            "5/5 - 0s - loss: 0.8000\n",
            "Epoch 7/100\n",
            "5/5 - 0s - loss: 0.7670\n",
            "Epoch 8/100\n",
            "5/5 - 0s - loss: 0.7410\n",
            "Epoch 9/100\n",
            "5/5 - 0s - loss: 0.7154\n",
            "Epoch 10/100\n",
            "5/5 - 0s - loss: 0.6960\n",
            "Epoch 11/100\n",
            "5/5 - 0s - loss: 0.6656\n",
            "Epoch 12/100\n",
            "5/5 - 0s - loss: 0.6398\n",
            "Epoch 13/100\n",
            "5/5 - 0s - loss: 0.6245\n",
            "Epoch 14/100\n",
            "5/5 - 0s - loss: 0.5925\n",
            "Epoch 15/100\n",
            "5/5 - 0s - loss: 0.5682\n",
            "Epoch 16/100\n",
            "5/5 - 0s - loss: 0.5491\n",
            "Epoch 17/100\n",
            "5/5 - 0s - loss: 0.5285\n",
            "Epoch 18/100\n",
            "5/5 - 0s - loss: 0.5163\n",
            "Epoch 19/100\n",
            "5/5 - 0s - loss: 0.4968\n",
            "Epoch 20/100\n",
            "5/5 - 0s - loss: 0.4826\n",
            "Epoch 21/100\n",
            "5/5 - 0s - loss: 0.4661\n",
            "Epoch 22/100\n",
            "5/5 - 0s - loss: 0.4560\n",
            "Epoch 23/100\n",
            "5/5 - 0s - loss: 0.4321\n",
            "Epoch 24/100\n",
            "5/5 - 0s - loss: 0.4172\n",
            "Epoch 25/100\n",
            "5/5 - 0s - loss: 0.4123\n",
            "Epoch 26/100\n",
            "5/5 - 0s - loss: 0.3988\n",
            "Epoch 27/100\n",
            "5/5 - 0s - loss: 0.3867\n",
            "Epoch 28/100\n",
            "5/5 - 0s - loss: 0.3709\n",
            "Epoch 29/100\n",
            "5/5 - 0s - loss: 0.3487\n",
            "Epoch 30/100\n",
            "5/5 - 0s - loss: 0.3448\n",
            "Epoch 31/100\n",
            "5/5 - 0s - loss: 0.3387\n",
            "Epoch 32/100\n",
            "5/5 - 0s - loss: 0.3158\n",
            "Epoch 33/100\n",
            "5/5 - 0s - loss: 0.3107\n",
            "Epoch 34/100\n",
            "5/5 - 0s - loss: 0.3038\n",
            "Epoch 35/100\n",
            "5/5 - 0s - loss: 0.2894\n",
            "Epoch 36/100\n",
            "5/5 - 0s - loss: 0.2800\n",
            "Epoch 37/100\n",
            "5/5 - 0s - loss: 0.2783\n",
            "Epoch 38/100\n",
            "5/5 - 0s - loss: 0.2638\n",
            "Epoch 39/100\n",
            "5/5 - 0s - loss: 0.2591\n",
            "Epoch 40/100\n",
            "5/5 - 0s - loss: 0.2471\n",
            "Epoch 41/100\n",
            "5/5 - 0s - loss: 0.2517\n",
            "Epoch 42/100\n",
            "5/5 - 0s - loss: 0.2388\n",
            "Epoch 43/100\n",
            "5/5 - 0s - loss: 0.2350\n",
            "Epoch 44/100\n",
            "5/5 - 0s - loss: 0.2250\n",
            "Epoch 45/100\n",
            "5/5 - 0s - loss: 0.2213\n",
            "Epoch 46/100\n",
            "5/5 - 0s - loss: 0.2137\n",
            "Epoch 47/100\n",
            "5/5 - 0s - loss: 0.2124\n",
            "Epoch 48/100\n",
            "5/5 - 0s - loss: 0.1996\n",
            "Epoch 49/100\n",
            "5/5 - 0s - loss: 0.2017\n",
            "Epoch 50/100\n",
            "5/5 - 0s - loss: 0.1902\n",
            "Epoch 51/100\n",
            "5/5 - 0s - loss: 0.1883\n",
            "Epoch 52/100\n",
            "5/5 - 0s - loss: 0.1799\n",
            "Epoch 53/100\n",
            "5/5 - 0s - loss: 0.1812\n",
            "Epoch 54/100\n",
            "5/5 - 0s - loss: 0.1768\n",
            "Epoch 55/100\n",
            "5/5 - 0s - loss: 0.1685\n",
            "Epoch 56/100\n",
            "5/5 - 0s - loss: 0.1667\n",
            "Epoch 57/100\n",
            "5/5 - 0s - loss: 0.1613\n",
            "Epoch 58/100\n",
            "5/5 - 0s - loss: 0.1612\n",
            "Epoch 59/100\n",
            "5/5 - 0s - loss: 0.1559\n",
            "Epoch 60/100\n",
            "5/5 - 0s - loss: 0.1557\n",
            "Epoch 61/100\n",
            "5/5 - 0s - loss: 0.1539\n",
            "Epoch 62/100\n",
            "5/5 - 0s - loss: 0.1492\n",
            "Epoch 63/100\n",
            "5/5 - 0s - loss: 0.1483\n",
            "Epoch 64/100\n",
            "5/5 - 0s - loss: 0.1405\n",
            "Epoch 65/100\n",
            "5/5 - 0s - loss: 0.1357\n",
            "Epoch 66/100\n",
            "5/5 - 0s - loss: 0.1339\n",
            "Epoch 67/100\n",
            "5/5 - 0s - loss: 0.1359\n",
            "Epoch 68/100\n",
            "5/5 - 0s - loss: 0.1304\n",
            "Epoch 69/100\n",
            "5/5 - 0s - loss: 0.1326\n",
            "Epoch 70/100\n",
            "5/5 - 0s - loss: 0.1225\n",
            "Epoch 71/100\n",
            "5/5 - 0s - loss: 0.1281\n",
            "Epoch 72/100\n",
            "5/5 - 0s - loss: 0.1306\n",
            "Epoch 73/100\n",
            "5/5 - 0s - loss: 0.1232\n",
            "Epoch 74/100\n",
            "5/5 - 0s - loss: 0.1239\n",
            "Epoch 75/100\n",
            "5/5 - 0s - loss: 0.1215\n",
            "Epoch 76/100\n",
            "5/5 - 0s - loss: 0.1141\n",
            "Epoch 77/100\n",
            "5/5 - 0s - loss: 0.1121\n",
            "Epoch 78/100\n",
            "5/5 - 0s - loss: 0.1139\n",
            "Epoch 79/100\n",
            "5/5 - 0s - loss: 0.1103\n",
            "Epoch 80/100\n",
            "5/5 - 0s - loss: 0.1115\n",
            "Epoch 81/100\n",
            "5/5 - 0s - loss: 0.1123\n",
            "Epoch 82/100\n",
            "5/5 - 0s - loss: 0.1040\n",
            "Epoch 83/100\n",
            "5/5 - 0s - loss: 0.1059\n",
            "Epoch 84/100\n",
            "5/5 - 0s - loss: 0.1127\n",
            "Epoch 85/100\n",
            "5/5 - 0s - loss: 0.1032\n",
            "Epoch 86/100\n",
            "5/5 - 0s - loss: 0.1078\n",
            "Epoch 87/100\n",
            "5/5 - 0s - loss: 0.0964\n",
            "Epoch 88/100\n",
            "5/5 - 0s - loss: 0.1034\n",
            "Epoch 89/100\n",
            "5/5 - 0s - loss: 0.0954\n",
            "Epoch 90/100\n",
            "5/5 - 0s - loss: 0.0996\n",
            "Epoch 91/100\n",
            "5/5 - 0s - loss: 0.0983\n",
            "Epoch 92/100\n",
            "5/5 - 0s - loss: 0.0963\n",
            "Epoch 93/100\n",
            "5/5 - 0s - loss: 0.0967\n",
            "Epoch 94/100\n",
            "5/5 - 0s - loss: 0.0941\n",
            "Epoch 95/100\n",
            "5/5 - 0s - loss: 0.0994\n",
            "Epoch 96/100\n",
            "5/5 - 0s - loss: 0.0951\n",
            "Epoch 97/100\n",
            "5/5 - 0s - loss: 0.0914\n",
            "Epoch 98/100\n",
            "5/5 - 0s - loss: 0.0907\n",
            "Epoch 99/100\n",
            "5/5 - 0s - loss: 0.0891\n",
            "Epoch 100/100\n",
            "5/5 - 0s - loss: 0.0908\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f35aa6bc128>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDCSM3ZsdSWh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "06267f5c-bbc2-4f43-bcd5-3c50ac09693f"
      },
      "source": [
        "# Print out number of species found:\n",
        "\n",
        "print(species)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'], dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkulh9CGdhBs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "faeb96ff-d52d-4ef1-a6aa-bbc468a12e57"
      },
      "source": [
        "pred = model.predict(x)\n",
        "print(f\"Shape: {pred.shape}\")\n",
        "print(pred[0:10])"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape: (150, 3)\n",
            "[[9.96951938e-01 3.04752053e-03 5.03024296e-07]\n",
            " [9.87803698e-01 1.21936183e-02 2.74413424e-06]\n",
            " [9.94865000e-01 5.13349427e-03 1.52783900e-06]\n",
            " [9.91327465e-01 8.66907276e-03 3.40549968e-06]\n",
            " [9.97881711e-01 2.11786921e-03 4.13901006e-07]\n",
            " [9.97808754e-01 2.19086674e-03 3.64135047e-07]\n",
            " [9.96516109e-01 3.48235318e-03 1.52564758e-06]\n",
            " [9.95526850e-01 4.47221473e-03 9.19154445e-07]\n",
            " [9.87828970e-01 1.21648405e-02 6.22018160e-06]\n",
            " [9.90911961e-01 9.08618420e-03 1.86287969e-06]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-fGOusqd5U4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If you would like to turn of scientific notation, the following \n",
        "# line can be used:\n",
        "np.set_printoptions(suppress=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDhmLsl-d79z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "fccb14bb-fe56-4413-c10c-ac63b8806019"
      },
      "source": [
        "# The to_xy function represented the input in the same way.  \n",
        "# Each row has only 1.0 value because each row is only one type\n",
        "# of iris.  This is the training data, we KNOW what type of iris it \n",
        "# is.  This is called one-hot encoding.  Only one value\n",
        "# is 1.0 (hot)\n",
        "print(y[0:10])"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 0 0]\n",
            " [1 0 0]\n",
            " [1 0 0]\n",
            " [1 0 0]\n",
            " [1 0 0]\n",
            " [1 0 0]\n",
            " [1 0 0]\n",
            " [1 0 0]\n",
            " [1 0 0]\n",
            " [1 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jrf-dpHMd_cD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "e57721fe-8cc3-4c1e-d604-a9c4a625e87c"
      },
      "source": [
        "\n",
        "# Usually the column (pred) with the highest prediction is considered \n",
        "# to be the prediction of the neural network.  It is easy\n",
        "# to convert the predictions to the expected iris species.  The argmax \n",
        "# function finds the index of the maximum prediction\n",
        "# for each row.\n",
        "predict_classes = np.argmax(pred,axis=1)\n",
        "expected_classes = np.argmax(y,axis=1)\n",
        "print(f\"Predictions: {predict_classes}\")\n",
        "print(f\"Expected: {expected_classes}\")"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predictions: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 1\n",
            " 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2]\n",
            "Expected: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFC1aNYEedCO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "4a7fa88c-5377-469b-ba89-7aa1ea881a93"
      },
      "source": [
        "# Of course it is very easy to turn these indexes back into iris species.  \n",
        "# We just use the species list that we created earlier.\n",
        "print(species[predict_classes[1:10]])"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa',\n",
            "       'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa',\n",
            "       'Iris-setosa'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTf6AoAEevC1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "11fa4700-74ec-4b31-8709-86ecec24cead"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "# Accuracy might be a more easily understood error metric.  It is \n",
        "# essentially a test score.  For all of the iris predictions,\n",
        "# what percent were correct?  The downside is it does not consider \n",
        "# how confident the neural network was in each prediction.\n",
        "correct = accuracy_score(expected_classes,predict_classes)\n",
        "print(f\"Accuracy: {correct}\")"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.98\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbzsnDh_e1Qq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6bb94591-6010-4bf2-b7d1-394ebacda367"
      },
      "source": [
        "# ad hoc prediction\n",
        "sample_flower = np.array( [[5.0,3.0,4.0,2.0]], dtype=float)\n",
        "pred = model.predict(sample_flower)\n",
        "print(pred)\n",
        "pred = np.argmax(pred)\n",
        "print(f\"Predict that {sample_flower} is: {species[pred]}\")"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.00420326 0.27269703 0.72309977]]\n",
            "Predict that [[5. 3. 4. 2.]] is: Iris-virginica\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnULBbwHfbkC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "9a541c53-ce50-4546-fe8e-f9c58cfe6111"
      },
      "source": [
        "# predict two sample flowers\n",
        "sample_flower = np.array( [[5.0,3.0,4.0,2.0],[5.2,3.5,1.5,0.8]], dtype=float)\n",
        "pred = model.predict(sample_flower)\n",
        "print(pred)\n",
        "pred = np.argmax(pred,axis=1)\n",
        "print(f\"Predict that these two flowers {sample_flower} are: {species[pred]}\")"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.00420326 0.27269703 0.72309977]\n",
            " [0.9908535  0.00914295 0.00000359]]\n",
            "Predict that these two flowers [[5.  3.  4.  2. ]\n",
            " [5.2 3.5 1.5 0.8]] are: Index(['Iris-virginica', 'Iris-setosa'], dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJXS06nnfc_e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Part 3.3: Saving and Loading a Keras Neural Network"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MflI-bQehSkE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "5f423b56-8459-46d5-ebbb-61d60160ae72"
      },
      "source": [
        "YAML - Stores the neural network structure (no weights) in the YAML file format.\n",
        "JSON - Stores the neural network structure (no weights) in the JSON file format.\n",
        "HDF5 - Stores the complete neural network (with weights) in the HDF5 file format. Do not confuse HDF5 with HDFS. They are different. We do not use HDFS in this class."
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-111-d1631417a49e>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    YAML - Stores the neural network structure (no weights) in the YAML file format.\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btIMF19khSpW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e394b9bf-f14c-414d-bc57-625262f97f67"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "import pandas as pd\n",
        "import io\n",
        "import os\n",
        "import requests\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "\n",
        "save_path = \".\"\n",
        "\n",
        "df = pd.read_csv(\n",
        "    \"https://data.heatonresearch.com/data/t81-558/auto-mpg.csv\", \n",
        "    na_values=['NA', '?'])\n",
        "\n",
        "cars = df['name']\n",
        "\n",
        "# Handle missing value\n",
        "df['horsepower'] = df['horsepower'].fillna(df['horsepower'].median())\n",
        "\n",
        "# Pandas to Numpy\n",
        "x = df[['cylinders', 'displacement', 'horsepower', 'weight',\n",
        "       'acceleration', 'year', 'origin']].values\n",
        "y = df['mpg'].values # regression\n",
        "\n",
        "# Build the neural network\n",
        "model = Sequential()\n",
        "model.add(Dense(25, input_dim=x.shape[1], activation='relu')) # Hidden 1\n",
        "model.add(Dense(10, activation='relu')) # Hidden 2\n",
        "model.add(Dense(1)) # Output\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "model.fit(x,y,verbose=2,epochs=100)\n",
        "\n",
        "# Predict\n",
        "pred = model.predict(x)\n",
        "\n",
        "# Measure RMSE error.  RMSE is common for regression.\n",
        "score = np.sqrt(metrics.mean_squared_error(pred,y))\n",
        "print(f\"Before save score (RMSE): {score}\")\n"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "13/13 - 0s - loss: 611318.3125\n",
            "Epoch 2/100\n",
            "13/13 - 0s - loss: 254016.8906\n",
            "Epoch 3/100\n",
            "13/13 - 0s - loss: 73524.7656\n",
            "Epoch 4/100\n",
            "13/13 - 0s - loss: 10914.5400\n",
            "Epoch 5/100\n",
            "13/13 - 0s - loss: 409.2942\n",
            "Epoch 6/100\n",
            "13/13 - 0s - loss: 713.1504\n",
            "Epoch 7/100\n",
            "13/13 - 0s - loss: 422.2583\n",
            "Epoch 8/100\n",
            "13/13 - 0s - loss: 118.2442\n",
            "Epoch 9/100\n",
            "13/13 - 0s - loss: 87.0941\n",
            "Epoch 10/100\n",
            "13/13 - 0s - loss: 85.6902\n",
            "Epoch 11/100\n",
            "13/13 - 0s - loss: 78.7966\n",
            "Epoch 12/100\n",
            "13/13 - 0s - loss: 79.6307\n",
            "Epoch 13/100\n",
            "13/13 - 0s - loss: 75.5242\n",
            "Epoch 14/100\n",
            "13/13 - 0s - loss: 75.4364\n",
            "Epoch 15/100\n",
            "13/13 - 0s - loss: 75.5056\n",
            "Epoch 16/100\n",
            "13/13 - 0s - loss: 74.0201\n",
            "Epoch 17/100\n",
            "13/13 - 0s - loss: 73.6731\n",
            "Epoch 18/100\n",
            "13/13 - 0s - loss: 73.7420\n",
            "Epoch 19/100\n",
            "13/13 - 0s - loss: 73.7782\n",
            "Epoch 20/100\n",
            "13/13 - 0s - loss: 71.9966\n",
            "Epoch 21/100\n",
            "13/13 - 0s - loss: 74.5107\n",
            "Epoch 22/100\n",
            "13/13 - 0s - loss: 70.6797\n",
            "Epoch 23/100\n",
            "13/13 - 0s - loss: 73.1980\n",
            "Epoch 24/100\n",
            "13/13 - 0s - loss: 72.1769\n",
            "Epoch 25/100\n",
            "13/13 - 0s - loss: 73.8204\n",
            "Epoch 26/100\n",
            "13/13 - 0s - loss: 70.3107\n",
            "Epoch 27/100\n",
            "13/13 - 0s - loss: 69.7247\n",
            "Epoch 28/100\n",
            "13/13 - 0s - loss: 70.3823\n",
            "Epoch 29/100\n",
            "13/13 - 0s - loss: 69.1697\n",
            "Epoch 30/100\n",
            "13/13 - 0s - loss: 69.5882\n",
            "Epoch 31/100\n",
            "13/13 - 0s - loss: 66.8418\n",
            "Epoch 32/100\n",
            "13/13 - 0s - loss: 65.7443\n",
            "Epoch 33/100\n",
            "13/13 - 0s - loss: 67.0409\n",
            "Epoch 34/100\n",
            "13/13 - 0s - loss: 69.1035\n",
            "Epoch 35/100\n",
            "13/13 - 0s - loss: 67.3176\n",
            "Epoch 36/100\n",
            "13/13 - 0s - loss: 65.1672\n",
            "Epoch 37/100\n",
            "13/13 - 0s - loss: 64.4341\n",
            "Epoch 38/100\n",
            "13/13 - 0s - loss: 62.5818\n",
            "Epoch 39/100\n",
            "13/13 - 0s - loss: 63.5208\n",
            "Epoch 40/100\n",
            "13/13 - 0s - loss: 65.2579\n",
            "Epoch 41/100\n",
            "13/13 - 0s - loss: 62.3271\n",
            "Epoch 42/100\n",
            "13/13 - 0s - loss: 61.9057\n",
            "Epoch 43/100\n",
            "13/13 - 0s - loss: 61.3570\n",
            "Epoch 44/100\n",
            "13/13 - 0s - loss: 61.4619\n",
            "Epoch 45/100\n",
            "13/13 - 0s - loss: 60.9458\n",
            "Epoch 46/100\n",
            "13/13 - 0s - loss: 59.8330\n",
            "Epoch 47/100\n",
            "13/13 - 0s - loss: 59.2267\n",
            "Epoch 48/100\n",
            "13/13 - 0s - loss: 59.5488\n",
            "Epoch 49/100\n",
            "13/13 - 0s - loss: 59.9634\n",
            "Epoch 50/100\n",
            "13/13 - 0s - loss: 59.0758\n",
            "Epoch 51/100\n",
            "13/13 - 0s - loss: 57.4105\n",
            "Epoch 52/100\n",
            "13/13 - 0s - loss: 57.1758\n",
            "Epoch 53/100\n",
            "13/13 - 0s - loss: 57.4406\n",
            "Epoch 54/100\n",
            "13/13 - 0s - loss: 57.6633\n",
            "Epoch 55/100\n",
            "13/13 - 0s - loss: 54.4558\n",
            "Epoch 56/100\n",
            "13/13 - 0s - loss: 55.6519\n",
            "Epoch 57/100\n",
            "13/13 - 0s - loss: 55.0962\n",
            "Epoch 58/100\n",
            "13/13 - 0s - loss: 53.6523\n",
            "Epoch 59/100\n",
            "13/13 - 0s - loss: 53.6522\n",
            "Epoch 60/100\n",
            "13/13 - 0s - loss: 52.4618\n",
            "Epoch 61/100\n",
            "13/13 - 0s - loss: 53.6934\n",
            "Epoch 62/100\n",
            "13/13 - 0s - loss: 51.1240\n",
            "Epoch 63/100\n",
            "13/13 - 0s - loss: 52.4152\n",
            "Epoch 64/100\n",
            "13/13 - 0s - loss: 51.1520\n",
            "Epoch 65/100\n",
            "13/13 - 0s - loss: 50.2080\n",
            "Epoch 66/100\n",
            "13/13 - 0s - loss: 49.6425\n",
            "Epoch 67/100\n",
            "13/13 - 0s - loss: 52.0274\n",
            "Epoch 68/100\n",
            "13/13 - 0s - loss: 49.0220\n",
            "Epoch 69/100\n",
            "13/13 - 0s - loss: 48.5435\n",
            "Epoch 70/100\n",
            "13/13 - 0s - loss: 48.1941\n",
            "Epoch 71/100\n",
            "13/13 - 0s - loss: 47.3491\n",
            "Epoch 72/100\n",
            "13/13 - 0s - loss: 46.4834\n",
            "Epoch 73/100\n",
            "13/13 - 0s - loss: 47.8288\n",
            "Epoch 74/100\n",
            "13/13 - 0s - loss: 48.3277\n",
            "Epoch 75/100\n",
            "13/13 - 0s - loss: 44.9401\n",
            "Epoch 76/100\n",
            "13/13 - 0s - loss: 45.5128\n",
            "Epoch 77/100\n",
            "13/13 - 0s - loss: 45.2574\n",
            "Epoch 78/100\n",
            "13/13 - 0s - loss: 44.4674\n",
            "Epoch 79/100\n",
            "13/13 - 0s - loss: 44.7974\n",
            "Epoch 80/100\n",
            "13/13 - 0s - loss: 46.6350\n",
            "Epoch 81/100\n",
            "13/13 - 0s - loss: 42.6821\n",
            "Epoch 82/100\n",
            "13/13 - 0s - loss: 42.6127\n",
            "Epoch 83/100\n",
            "13/13 - 0s - loss: 43.1064\n",
            "Epoch 84/100\n",
            "13/13 - 0s - loss: 43.7870\n",
            "Epoch 85/100\n",
            "13/13 - 0s - loss: 41.1231\n",
            "Epoch 86/100\n",
            "13/13 - 0s - loss: 40.2598\n",
            "Epoch 87/100\n",
            "13/13 - 0s - loss: 39.9243\n",
            "Epoch 88/100\n",
            "13/13 - 0s - loss: 39.8468\n",
            "Epoch 89/100\n",
            "13/13 - 0s - loss: 39.0571\n",
            "Epoch 90/100\n",
            "13/13 - 0s - loss: 38.8372\n",
            "Epoch 91/100\n",
            "13/13 - 0s - loss: 40.0105\n",
            "Epoch 92/100\n",
            "13/13 - 0s - loss: 38.3621\n",
            "Epoch 93/100\n",
            "13/13 - 0s - loss: 39.4243\n",
            "Epoch 94/100\n",
            "13/13 - 0s - loss: 37.4240\n",
            "Epoch 95/100\n",
            "13/13 - 0s - loss: 36.7511\n",
            "Epoch 96/100\n",
            "13/13 - 0s - loss: 37.5795\n",
            "Epoch 97/100\n",
            "13/13 - 0s - loss: 35.8785\n",
            "Epoch 98/100\n",
            "13/13 - 0s - loss: 36.0477\n",
            "Epoch 99/100\n",
            "13/13 - 0s - loss: 37.7643\n",
            "Epoch 100/100\n",
            "13/13 - 0s - loss: 35.3440\n",
            "Before save score (RMSE): 5.956472878219739\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ac7w7LYDfkVi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save neural network structure to JSON (no weights)\n",
        "\n",
        "save_path = \".\"\n",
        "\n",
        "model_json = mymodel.to_json()\n",
        "with open(os.path.join(save_path,\"network.json\"), \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "# save neural network structure to YAML (no weights)\n",
        "model_yaml = mymodel.to_yaml()\n",
        "with open(os.path.join(save_path,\"network.yaml\"), \"w\") as yaml_file:\n",
        "    yaml_file.write(model_yaml)\n",
        "\n",
        "# save entire network to HDF5 (save everything, suggested)\n",
        "mymodel.save(os.path.join(save_path,\"network.h5\"))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bb-BrvtVgNzj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Now we reload the network and perform another prediction. The RMSE should match the previous one exactly if the neural network was really saved and reloaded."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWqf5iHLgVfU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a576a9a7-3c42-499e-b748-b8d617456eaa"
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "model2 = load_model(os.path.join(save_path,\"network.h5\"))\n",
        "pred = model2.predict(x)\n",
        "# Measure RMSE error.  RMSE is common for regression.\n",
        "score = np.sqrt(metrics.mean_squared_error(pred,y))\n",
        "print(f\"After load score (RMSE): {score}\")"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After load score (RMSE): 13.358812495721514\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvDFgjEtgtbq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Part 3.4: Early Stopping in Keras to Prevent Overfitting"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RY9FR-ZsgvPu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io\n",
        "import requests\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "df = pd.read_csv(\n",
        "    \"https://data.heatonresearch.com/data/t81-558/iris.csv\", \n",
        "    na_values=['NA', '?'])\n",
        "\n",
        "# Convert to numpy - Classification\n",
        "x = df[['sepal_l', 'sepal_w', 'petal_l', 'petal_w']].values\n",
        "dummies = pd.get_dummies(df['species']) # Classification\n",
        "species = dummies.columns\n",
        "y = dummies.values\n",
        "\n",
        "# Split into validation and training sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(    \n",
        "    x, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Build neural network\n",
        "model = Sequential()\n",
        "model.add(Dense(50, input_dim=x.shape[1], activation='relu')) # Hidden 1\n",
        "model.add(Dense(25, activation='relu')) # Hidden 2\n",
        "model.add(Dense(y.shape[1],activation='softmax')) # Output\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDDqtG5IiCIP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "outputId": "030dcafd-b9fd-4721-ff7f-a4f9cf8a7d0d"
      },
      "source": [
        "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, \n",
        "        verbose=1, mode='auto', restore_best_weights=True)\n",
        "model.fit(x_train,y_train,validation_data=(x_test,y_test),\n",
        "        callbacks=[monitor],verbose=1,epochs=1000)"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0827 - val_loss: 0.0540\n",
            "Epoch 2/1000\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0779 - val_loss: 0.0574\n",
            "Epoch 3/1000\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0781 - val_loss: 0.0527\n",
            "Epoch 4/1000\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0793 - val_loss: 0.0520\n",
            "Epoch 5/1000\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0955 - val_loss: 0.0510\n",
            "Epoch 6/1000\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.0821 - val_loss: 0.0553\n",
            "Epoch 7/1000\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0841 - val_loss: 0.0602\n",
            "Epoch 8/1000\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0820 - val_loss: 0.0498\n",
            "Epoch 9/1000\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0767 - val_loss: 0.0491\n",
            "Epoch 10/1000\n",
            "4/4 [==============================] - 0s 50ms/step - loss: 0.0908 - val_loss: 0.0522\n",
            "Epoch 11/1000\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0937 - val_loss: 0.0501\n",
            "Epoch 12/1000\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0759 - val_loss: 0.0465\n",
            "Epoch 13/1000\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.0796 - val_loss: 0.0461\n",
            "Epoch 14/1000\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0776 - val_loss: 0.0492\n",
            "Epoch 15/1000\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0796 - val_loss: 0.0488\n",
            "Epoch 16/1000\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0755 - val_loss: 0.0475\n",
            "Epoch 17/1000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.0687Restoring model weights from the end of the best epoch.\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0738 - val_loss: 0.0479\n",
            "Epoch 00017: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f35a10c1e80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ktuFADviFtM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "07e4f082-42b1-4be7-f505-acafd656da70"
      },
      "source": [
        "There are a number of parameters that are specified to the EarlyStopping object.\n",
        "\n",
        "min_delta This value should be kept small. It simply means the minimum change in error to be registered as an improvement. Setting it even smaller will not likely have a great deal of impact.\n",
        "patience How long should the training wait for the validation error to improve?\n",
        "verbose How much progress information do you want?\n",
        "mode In general, always set this to \"auto\". This allows you to specify if the error should be minimized or maximized. Consider accuracy, where higher numbers are desired vs log-loss/RMSE where lower numbers are desired.\n",
        "restore_best_weights This should always be set to true. This restores the weights to the values they were at when the validation set is the highest. Unless you are manually tracking the weights yourself (we do not use this technique in this course), you should have Keras perform this step for you.\n",
        "As you can see from above, the entire number of requested epochs were not used. The neural network training stopped once the validation set no longer improved."
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-124-ccda5dd66c1d>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    There are a number of parameters that are specified to the EarlyStopping object.\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01V8nOSCjF7y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cb543e43-8d36-4475-a627-6692554148f8"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "pred = model.predict(x_test)\n",
        "predict_classes = np.argmax(pred,axis=1)\n",
        "expected_classes = np.argmax(y_test,axis=1)\n",
        "correct = accuracy_score(expected_classes,predict_classes)\n",
        "print(f\"Accuracy: {correct}\")"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9736842105263158\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGoMXFRgjL0l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e7f13d7e-b379-4044-8575-a92b89f90eb1"
      },
      "source": [
        "## Early Stopping with Regression\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "import pandas as pd\n",
        "import io\n",
        "import os\n",
        "import requests\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "\n",
        "df = pd.read_csv(\n",
        "    \"https://data.heatonresearch.com/data/t81-558/auto-mpg.csv\", \n",
        "    na_values=['NA', '?'])\n",
        "\n",
        "cars = df['name']\n",
        "\n",
        "# Handle missing value\n",
        "df['horsepower'] = df['horsepower'].fillna(df['horsepower'].median())\n",
        "\n",
        "# Pandas to Numpy\n",
        "x = df[['cylinders', 'displacement', 'horsepower', 'weight',\n",
        "       'acceleration', 'year', 'origin']].values\n",
        "y = df['mpg'].values # regression\n",
        "\n",
        "# Split into validation and training sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(    \n",
        "    x, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Build the neural network\n",
        "model = Sequential()\n",
        "model.add(Dense(25, input_dim=x.shape[1], activation='relu')) # Hidden 1\n",
        "model.add(Dense(10, activation='relu')) # Hidden 2\n",
        "model.add(Dense(1)) # Output\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, \n",
        "        patience=5, verbose=1, mode='auto',\n",
        "        restore_best_weights=True)\n",
        "model.fit(x_train,y_train,validation_data=(x_test,y_test),\n",
        "        callbacks=[monitor], verbose=2,epochs=1000)"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "10/10 - 0s - loss: 19868.8066 - val_loss: 809.3716\n",
            "Epoch 2/1000\n",
            "10/10 - 0s - loss: 2393.6631 - val_loss: 4335.1992\n",
            "Epoch 3/1000\n",
            "10/10 - 0s - loss: 2310.3879 - val_loss: 576.1735\n",
            "Epoch 4/1000\n",
            "10/10 - 0s - loss: 822.4606 - val_loss: 938.7767\n",
            "Epoch 5/1000\n",
            "10/10 - 0s - loss: 679.5529 - val_loss: 430.1645\n",
            "Epoch 6/1000\n",
            "10/10 - 0s - loss: 441.7359 - val_loss: 414.9118\n",
            "Epoch 7/1000\n",
            "10/10 - 0s - loss: 421.7202 - val_loss: 400.1056\n",
            "Epoch 8/1000\n",
            "10/10 - 0s - loss: 416.5208 - val_loss: 392.1399\n",
            "Epoch 9/1000\n",
            "10/10 - 0s - loss: 399.3857 - val_loss: 375.2391\n",
            "Epoch 10/1000\n",
            "10/10 - 0s - loss: 376.6493 - val_loss: 362.7425\n",
            "Epoch 11/1000\n",
            "10/10 - 0s - loss: 369.5479 - val_loss: 353.9958\n",
            "Epoch 12/1000\n",
            "10/10 - 0s - loss: 373.4594 - val_loss: 339.8678\n",
            "Epoch 13/1000\n",
            "10/10 - 0s - loss: 358.4051 - val_loss: 332.0164\n",
            "Epoch 14/1000\n",
            "10/10 - 0s - loss: 352.1795 - val_loss: 318.5224\n",
            "Epoch 15/1000\n",
            "10/10 - 0s - loss: 330.8531 - val_loss: 305.0433\n",
            "Epoch 16/1000\n",
            "10/10 - 0s - loss: 332.5830 - val_loss: 294.1615\n",
            "Epoch 17/1000\n",
            "10/10 - 0s - loss: 315.5147 - val_loss: 287.8111\n",
            "Epoch 18/1000\n",
            "10/10 - 0s - loss: 302.5817 - val_loss: 272.6648\n",
            "Epoch 19/1000\n",
            "10/10 - 0s - loss: 280.2783 - val_loss: 262.2368\n",
            "Epoch 20/1000\n",
            "10/10 - 0s - loss: 273.7745 - val_loss: 253.2786\n",
            "Epoch 21/1000\n",
            "10/10 - 0s - loss: 266.6205 - val_loss: 242.4106\n",
            "Epoch 22/1000\n",
            "10/10 - 0s - loss: 247.4535 - val_loss: 232.6046\n",
            "Epoch 23/1000\n",
            "10/10 - 0s - loss: 247.7238 - val_loss: 230.2439\n",
            "Epoch 24/1000\n",
            "10/10 - 0s - loss: 260.9181 - val_loss: 215.6474\n",
            "Epoch 25/1000\n",
            "10/10 - 0s - loss: 238.8197 - val_loss: 214.7499\n",
            "Epoch 26/1000\n",
            "10/10 - 0s - loss: 225.9790 - val_loss: 214.6640\n",
            "Epoch 27/1000\n",
            "10/10 - 0s - loss: 219.1623 - val_loss: 189.3311\n",
            "Epoch 28/1000\n",
            "10/10 - 0s - loss: 204.7062 - val_loss: 182.3992\n",
            "Epoch 29/1000\n",
            "10/10 - 0s - loss: 205.9181 - val_loss: 182.5570\n",
            "Epoch 30/1000\n",
            "10/10 - 0s - loss: 202.4646 - val_loss: 177.8407\n",
            "Epoch 31/1000\n",
            "10/10 - 0s - loss: 186.0609 - val_loss: 164.8427\n",
            "Epoch 32/1000\n",
            "10/10 - 0s - loss: 185.7881 - val_loss: 152.7189\n",
            "Epoch 33/1000\n",
            "10/10 - 0s - loss: 172.1720 - val_loss: 153.2374\n",
            "Epoch 34/1000\n",
            "10/10 - 0s - loss: 170.0086 - val_loss: 140.3378\n",
            "Epoch 35/1000\n",
            "10/10 - 0s - loss: 162.4294 - val_loss: 138.4846\n",
            "Epoch 36/1000\n",
            "10/10 - 0s - loss: 158.5861 - val_loss: 137.9976\n",
            "Epoch 37/1000\n",
            "10/10 - 0s - loss: 156.8303 - val_loss: 125.1284\n",
            "Epoch 38/1000\n",
            "10/10 - 0s - loss: 137.5614 - val_loss: 123.7104\n",
            "Epoch 39/1000\n",
            "10/10 - 0s - loss: 136.5400 - val_loss: 135.1861\n",
            "Epoch 40/1000\n",
            "10/10 - 0s - loss: 150.1800 - val_loss: 111.3260\n",
            "Epoch 41/1000\n",
            "10/10 - 0s - loss: 142.8156 - val_loss: 112.5024\n",
            "Epoch 42/1000\n",
            "10/10 - 0s - loss: 135.1906 - val_loss: 103.5140\n",
            "Epoch 43/1000\n",
            "10/10 - 0s - loss: 128.7242 - val_loss: 101.4726\n",
            "Epoch 44/1000\n",
            "10/10 - 0s - loss: 120.9890 - val_loss: 95.1731\n",
            "Epoch 45/1000\n",
            "10/10 - 0s - loss: 122.6489 - val_loss: 91.9675\n",
            "Epoch 46/1000\n",
            "10/10 - 0s - loss: 119.5906 - val_loss: 102.2021\n",
            "Epoch 47/1000\n",
            "10/10 - 0s - loss: 113.6129 - val_loss: 89.7355\n",
            "Epoch 48/1000\n",
            "10/10 - 0s - loss: 109.6671 - val_loss: 84.9731\n",
            "Epoch 49/1000\n",
            "10/10 - 0s - loss: 114.4253 - val_loss: 82.0867\n",
            "Epoch 50/1000\n",
            "10/10 - 0s - loss: 106.1898 - val_loss: 82.8733\n",
            "Epoch 51/1000\n",
            "10/10 - 0s - loss: 102.8870 - val_loss: 77.6903\n",
            "Epoch 52/1000\n",
            "10/10 - 0s - loss: 100.4265 - val_loss: 75.2960\n",
            "Epoch 53/1000\n",
            "10/10 - 0s - loss: 100.8082 - val_loss: 87.3359\n",
            "Epoch 54/1000\n",
            "10/10 - 0s - loss: 98.8356 - val_loss: 80.1860\n",
            "Epoch 55/1000\n",
            "10/10 - 0s - loss: 102.8792 - val_loss: 76.8221\n",
            "Epoch 56/1000\n",
            "10/10 - 0s - loss: 107.7229 - val_loss: 71.0332\n",
            "Epoch 57/1000\n",
            "10/10 - 0s - loss: 100.1249 - val_loss: 73.0953\n",
            "Epoch 58/1000\n",
            "10/10 - 0s - loss: 98.5642 - val_loss: 66.8082\n",
            "Epoch 59/1000\n",
            "10/10 - 0s - loss: 97.5957 - val_loss: 90.0145\n",
            "Epoch 60/1000\n",
            "10/10 - 0s - loss: 94.7414 - val_loss: 70.8359\n",
            "Epoch 61/1000\n",
            "10/10 - 0s - loss: 90.8287 - val_loss: 64.9500\n",
            "Epoch 62/1000\n",
            "10/10 - 0s - loss: 89.6031 - val_loss: 63.4462\n",
            "Epoch 63/1000\n",
            "10/10 - 0s - loss: 88.8760 - val_loss: 66.6014\n",
            "Epoch 64/1000\n",
            "10/10 - 0s - loss: 107.8571 - val_loss: 68.2740\n",
            "Epoch 65/1000\n",
            "10/10 - 0s - loss: 94.0805 - val_loss: 66.4080\n",
            "Epoch 66/1000\n",
            "10/10 - 0s - loss: 88.0043 - val_loss: 61.4635\n",
            "Epoch 67/1000\n",
            "10/10 - 0s - loss: 87.3304 - val_loss: 63.2497\n",
            "Epoch 68/1000\n",
            "10/10 - 0s - loss: 84.0702 - val_loss: 59.2788\n",
            "Epoch 69/1000\n",
            "10/10 - 0s - loss: 83.5849 - val_loss: 65.7515\n",
            "Epoch 70/1000\n",
            "10/10 - 0s - loss: 82.9307 - val_loss: 56.7419\n",
            "Epoch 71/1000\n",
            "10/10 - 0s - loss: 83.2903 - val_loss: 62.1983\n",
            "Epoch 72/1000\n",
            "10/10 - 0s - loss: 79.8737 - val_loss: 55.6436\n",
            "Epoch 73/1000\n",
            "10/10 - 0s - loss: 81.2333 - val_loss: 66.0017\n",
            "Epoch 74/1000\n",
            "10/10 - 0s - loss: 76.9267 - val_loss: 55.0101\n",
            "Epoch 75/1000\n",
            "10/10 - 0s - loss: 77.4451 - val_loss: 57.3425\n",
            "Epoch 76/1000\n",
            "10/10 - 0s - loss: 81.3639 - val_loss: 54.1886\n",
            "Epoch 77/1000\n",
            "10/10 - 0s - loss: 75.3973 - val_loss: 73.5067\n",
            "Epoch 78/1000\n",
            "10/10 - 0s - loss: 77.5693 - val_loss: 71.7724\n",
            "Epoch 79/1000\n",
            "10/10 - 0s - loss: 85.0366 - val_loss: 51.8440\n",
            "Epoch 80/1000\n",
            "10/10 - 0s - loss: 83.7251 - val_loss: 51.6425\n",
            "Epoch 81/1000\n",
            "10/10 - 0s - loss: 72.1450 - val_loss: 55.2420\n",
            "Epoch 82/1000\n",
            "10/10 - 0s - loss: 74.6659 - val_loss: 53.9121\n",
            "Epoch 83/1000\n",
            "10/10 - 0s - loss: 82.1995 - val_loss: 51.9685\n",
            "Epoch 84/1000\n",
            "10/10 - 0s - loss: 77.2128 - val_loss: 49.7585\n",
            "Epoch 85/1000\n",
            "10/10 - 0s - loss: 71.8610 - val_loss: 51.7122\n",
            "Epoch 86/1000\n",
            "10/10 - 0s - loss: 76.5628 - val_loss: 49.2202\n",
            "Epoch 87/1000\n",
            "10/10 - 0s - loss: 72.4701 - val_loss: 49.8876\n",
            "Epoch 88/1000\n",
            "10/10 - 0s - loss: 67.6652 - val_loss: 48.3593\n",
            "Epoch 89/1000\n",
            "10/10 - 0s - loss: 69.8848 - val_loss: 48.3606\n",
            "Epoch 90/1000\n",
            "10/10 - 0s - loss: 69.3905 - val_loss: 46.8714\n",
            "Epoch 91/1000\n",
            "10/10 - 0s - loss: 74.6517 - val_loss: 57.5376\n",
            "Epoch 92/1000\n",
            "10/10 - 0s - loss: 71.3732 - val_loss: 56.9023\n",
            "Epoch 93/1000\n",
            "10/10 - 0s - loss: 68.7552 - val_loss: 45.8263\n",
            "Epoch 94/1000\n",
            "10/10 - 0s - loss: 65.5205 - val_loss: 55.9038\n",
            "Epoch 95/1000\n",
            "10/10 - 0s - loss: 72.9901 - val_loss: 62.7202\n",
            "Epoch 96/1000\n",
            "10/10 - 0s - loss: 65.8088 - val_loss: 48.4314\n",
            "Epoch 97/1000\n",
            "10/10 - 0s - loss: 62.4309 - val_loss: 43.8864\n",
            "Epoch 98/1000\n",
            "10/10 - 0s - loss: 64.3608 - val_loss: 49.6273\n",
            "Epoch 99/1000\n",
            "10/10 - 0s - loss: 63.6594 - val_loss: 43.2979\n",
            "Epoch 100/1000\n",
            "10/10 - 0s - loss: 62.1003 - val_loss: 42.6957\n",
            "Epoch 101/1000\n",
            "10/10 - 0s - loss: 64.9475 - val_loss: 44.5771\n",
            "Epoch 102/1000\n",
            "10/10 - 0s - loss: 65.9043 - val_loss: 42.0404\n",
            "Epoch 103/1000\n",
            "10/10 - 0s - loss: 64.0283 - val_loss: 44.5662\n",
            "Epoch 104/1000\n",
            "10/10 - 0s - loss: 59.9855 - val_loss: 44.0833\n",
            "Epoch 105/1000\n",
            "10/10 - 0s - loss: 57.7829 - val_loss: 44.7386\n",
            "Epoch 106/1000\n",
            "10/10 - 0s - loss: 61.8577 - val_loss: 47.5005\n",
            "Epoch 107/1000\n",
            "10/10 - 0s - loss: 56.8739 - val_loss: 40.0232\n",
            "Epoch 108/1000\n",
            "10/10 - 0s - loss: 57.2923 - val_loss: 39.7345\n",
            "Epoch 109/1000\n",
            "10/10 - 0s - loss: 56.4539 - val_loss: 40.7641\n",
            "Epoch 110/1000\n",
            "10/10 - 0s - loss: 59.8756 - val_loss: 41.9866\n",
            "Epoch 111/1000\n",
            "10/10 - 0s - loss: 66.1131 - val_loss: 49.3089\n",
            "Epoch 112/1000\n",
            "10/10 - 0s - loss: 58.0746 - val_loss: 44.0897\n",
            "Epoch 113/1000\n",
            "10/10 - 0s - loss: 55.3777 - val_loss: 38.3153\n",
            "Epoch 114/1000\n",
            "10/10 - 0s - loss: 55.0044 - val_loss: 37.6147\n",
            "Epoch 115/1000\n",
            "10/10 - 0s - loss: 62.3617 - val_loss: 49.0861\n",
            "Epoch 116/1000\n",
            "10/10 - 0s - loss: 59.5126 - val_loss: 43.3966\n",
            "Epoch 117/1000\n",
            "10/10 - 0s - loss: 55.7104 - val_loss: 38.2950\n",
            "Epoch 118/1000\n",
            "10/10 - 0s - loss: 55.5836 - val_loss: 37.1624\n",
            "Epoch 119/1000\n",
            "10/10 - 0s - loss: 55.1566 - val_loss: 43.7712\n",
            "Epoch 120/1000\n",
            "10/10 - 0s - loss: 54.6187 - val_loss: 36.9507\n",
            "Epoch 121/1000\n",
            "10/10 - 0s - loss: 51.3859 - val_loss: 35.7778\n",
            "Epoch 122/1000\n",
            "10/10 - 0s - loss: 54.2351 - val_loss: 40.6684\n",
            "Epoch 123/1000\n",
            "10/10 - 0s - loss: 58.5495 - val_loss: 45.4980\n",
            "Epoch 124/1000\n",
            "10/10 - 0s - loss: 50.0269 - val_loss: 34.4032\n",
            "Epoch 125/1000\n",
            "10/10 - 0s - loss: 50.2945 - val_loss: 39.5433\n",
            "Epoch 126/1000\n",
            "10/10 - 0s - loss: 54.4970 - val_loss: 33.5841\n",
            "Epoch 127/1000\n",
            "10/10 - 0s - loss: 48.3919 - val_loss: 33.3446\n",
            "Epoch 128/1000\n",
            "10/10 - 0s - loss: 47.9946 - val_loss: 32.8935\n",
            "Epoch 129/1000\n",
            "10/10 - 0s - loss: 48.0536 - val_loss: 33.8705\n",
            "Epoch 130/1000\n",
            "10/10 - 0s - loss: 49.1949 - val_loss: 32.6672\n",
            "Epoch 131/1000\n",
            "10/10 - 0s - loss: 46.2584 - val_loss: 33.3649\n",
            "Epoch 132/1000\n",
            "10/10 - 0s - loss: 46.4059 - val_loss: 38.5162\n",
            "Epoch 133/1000\n",
            "10/10 - 0s - loss: 47.6460 - val_loss: 31.9303\n",
            "Epoch 134/1000\n",
            "10/10 - 0s - loss: 45.8500 - val_loss: 36.2186\n",
            "Epoch 135/1000\n",
            "10/10 - 0s - loss: 46.8269 - val_loss: 37.7066\n",
            "Epoch 136/1000\n",
            "10/10 - 0s - loss: 53.2084 - val_loss: 49.1160\n",
            "Epoch 137/1000\n",
            "10/10 - 0s - loss: 48.7836 - val_loss: 57.4905\n",
            "Epoch 138/1000\n",
            "Restoring model weights from the end of the best epoch.\n",
            "10/10 - 0s - loss: 53.3758 - val_loss: 38.1582\n",
            "Epoch 00138: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f359f846748>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4j153nqjamz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bfb033df-ccdc-4f60-e966-8d98ace9f793"
      },
      "source": [
        "# Measure RMSE error.  RMSE is common for regression.\n",
        "pred = model.predict(x_test)\n",
        "score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
        "print(f\"Final score (RMSE): {score}\")"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final score (RMSE): 6.245712067609764\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HW7sdXjjevw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Part 3.5: Extracting Keras Weights and Manual Neural Network Calculation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IT1_DMrajokU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Manual Neural Network Calculation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzWOLf-VjxbP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "1680458a-7a8f-4176-e577-d660cbda3ae6"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "import numpy as np\n",
        "\n",
        "# Create a dataset for the XOR function\n",
        "x = np.array([\n",
        "    [0,0],\n",
        "    [1,0],\n",
        "    [0,1],\n",
        "    [1,1]\n",
        "])\n",
        "\n",
        "y = np.array([\n",
        "    0,\n",
        "    1,\n",
        "    1,\n",
        "    0\n",
        "])\n",
        "\n",
        "# Build the network\n",
        "# sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "done = False\n",
        "cycle = 1\n",
        "\n",
        "while not done:\n",
        "    print(\"Cycle #{}\".format(cycle))\n",
        "    cycle+=1\n",
        "    model = Sequential()\n",
        "    model.add(Dense(2, input_dim=2, activation='relu')) \n",
        "    model.add(Dense(1)) \n",
        "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "    model.fit(x,y,verbose=0,epochs=10000)\n",
        "\n",
        "    # Predict\n",
        "    pred = model.predict(x)\n",
        "    \n",
        "    # Check if successful.  It takes several runs with this \n",
        "    # small of a network\n",
        "    done = pred[0]<0.01 and pred[3]<0.01 and pred[1] > 0.9 \n",
        "        and pred[2] > 0.9 \n",
        "    print(pred)"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-130-3d43d9a431a0>\"\u001b[0;36m, line \u001b[0;32m41\u001b[0m\n\u001b[0;31m    and pred[2] > 0.9\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-EYX7vYj0vO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f0aa9f3d-a1b0-4988-d65a-fe5159a89a10"
      },
      "source": [
        "pred[3]\n"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([16.277758], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yWflusWkGTy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "676b995d-9c36-4dcf-a493-eb0999ae5d37"
      },
      "source": [
        "model.layers.count"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function list.count>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LB9wFBk6j3Na",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5c6fd53f-92cd-4775-b8f2-555ca2f5c399"
      },
      "source": [
        "# Dump weights\n",
        "for layerNum, layer in enumerate(model.layers):\n",
        "    weights = layer.get_weights()[0]\n",
        "    biases = layer.get_weights()[1]\n",
        "   \n",
        "    for toNeuronNum, bias in enumerate(biases):\n",
        "        print(f'{layerNum}B -> L{layerNum+1}N{toNeuronNum}: {bias}')\n",
        "    \n",
        "    for fromNeuronNum, wgt in enumerate(weights):\n",
        "        for toNeuronNum, wgt2 in enumerate(wgt):\n",
        "            print(f'L{layerNum}N{fromNeuronNum} \\\n",
        "                  -> L{layerNum+1}N{toNeuronNum} = {wgt2}')"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0B -> L1N0: -0.2351851463317871\n",
            "0B -> L1N1: 0.0\n",
            "0B -> L1N2: 0.0\n",
            "0B -> L1N3: 0.0\n",
            "0B -> L1N4: 0.0\n",
            "0B -> L1N5: 0.0\n",
            "0B -> L1N6: -0.14134375751018524\n",
            "0B -> L1N7: 0.4202514886856079\n",
            "0B -> L1N8: 0.0\n",
            "0B -> L1N9: 0.0\n",
            "0B -> L1N10: 0.0595504567027092\n",
            "0B -> L1N11: 0.10451910644769669\n",
            "0B -> L1N12: 0.0\n",
            "0B -> L1N13: 0.0\n",
            "0B -> L1N14: -0.09297453612089157\n",
            "0B -> L1N15: 0.0\n",
            "0B -> L1N16: 0.2188505381345749\n",
            "0B -> L1N17: 0.0\n",
            "0B -> L1N18: 0.04406147822737694\n",
            "0B -> L1N19: 0.0\n",
            "0B -> L1N20: 0.19929097592830658\n",
            "0B -> L1N21: -0.044772859662771225\n",
            "0B -> L1N22: 0.0\n",
            "0B -> L1N23: 0.10987051576375961\n",
            "0B -> L1N24: 0.0\n",
            "L0N0                   -> L1N0 = -0.019393129274249077\n",
            "L0N0                   -> L1N1 = 0.3180391490459442\n",
            "L0N0                   -> L1N2 = -0.044329285621643066\n",
            "L0N0                   -> L1N3 = 0.24383041262626648\n",
            "L0N0                   -> L1N4 = 0.3136233985424042\n",
            "L0N0                   -> L1N5 = -0.2881394624710083\n",
            "L0N0                   -> L1N6 = 0.15407685935497284\n",
            "L0N0                   -> L1N7 = 0.3816641569137573\n",
            "L0N0                   -> L1N8 = 0.39067748188972473\n",
            "L0N0                   -> L1N9 = -0.3192187249660492\n",
            "L0N0                   -> L1N10 = -0.2249060422182083\n",
            "L0N0                   -> L1N11 = -0.28238603472709656\n",
            "L0N0                   -> L1N12 = 0.3614636957645416\n",
            "L0N0                   -> L1N13 = 0.1117885410785675\n",
            "L0N0                   -> L1N14 = -0.014667248353362083\n",
            "L0N0                   -> L1N15 = -0.37944936752319336\n",
            "L0N0                   -> L1N16 = -0.34617146849632263\n",
            "L0N0                   -> L1N17 = 0.027082175016403198\n",
            "L0N0                   -> L1N18 = 0.074226014316082\n",
            "L0N0                   -> L1N19 = 0.33672502636909485\n",
            "L0N0                   -> L1N20 = 0.4270404875278473\n",
            "L0N0                   -> L1N21 = -0.28828442096710205\n",
            "L0N0                   -> L1N22 = 0.26331427693367004\n",
            "L0N0                   -> L1N23 = -0.17039655148983002\n",
            "L0N0                   -> L1N24 = 0.13932058215141296\n",
            "L0N1                   -> L1N0 = 0.40252724289894104\n",
            "L0N1                   -> L1N1 = -0.40698909759521484\n",
            "L0N1                   -> L1N2 = -0.20382754504680634\n",
            "L0N1                   -> L1N3 = 0.05203065276145935\n",
            "L0N1                   -> L1N4 = -0.015337914228439331\n",
            "L0N1                   -> L1N5 = 0.08988109230995178\n",
            "L0N1                   -> L1N6 = 0.03155514597892761\n",
            "L0N1                   -> L1N7 = 0.06297362595796585\n",
            "L0N1                   -> L1N8 = -0.22819364070892334\n",
            "L0N1                   -> L1N9 = -0.3044477105140686\n",
            "L0N1                   -> L1N10 = -0.144436776638031\n",
            "L0N1                   -> L1N11 = -0.20374611020088196\n",
            "L0N1                   -> L1N12 = -0.14497151970863342\n",
            "L0N1                   -> L1N13 = -0.32426077127456665\n",
            "L0N1                   -> L1N14 = -0.2810378968715668\n",
            "L0N1                   -> L1N15 = -0.36609572172164917\n",
            "L0N1                   -> L1N16 = -0.2007497102022171\n",
            "L0N1                   -> L1N17 = 0.17272844910621643\n",
            "L0N1                   -> L1N18 = 0.12042825669050217\n",
            "L0N1                   -> L1N19 = -0.00785776972770691\n",
            "L0N1                   -> L1N20 = -0.3417833745479584\n",
            "L0N1                   -> L1N21 = 0.2074018269777298\n",
            "L0N1                   -> L1N22 = 0.05530142784118652\n",
            "L0N1                   -> L1N23 = -0.057301003485918045\n",
            "L0N1                   -> L1N24 = -0.40950047969818115\n",
            "L0N2                   -> L1N0 = 0.256727397441864\n",
            "L0N2                   -> L1N1 = -0.14951369166374207\n",
            "L0N2                   -> L1N2 = -0.32515937089920044\n",
            "L0N2                   -> L1N3 = -0.022879570722579956\n",
            "L0N2                   -> L1N4 = -0.312778115272522\n",
            "L0N2                   -> L1N5 = 0.2953760325908661\n",
            "L0N2                   -> L1N6 = 0.3920813202857971\n",
            "L0N2                   -> L1N7 = -0.18182270228862762\n",
            "L0N2                   -> L1N8 = -0.16338509321212769\n",
            "L0N2                   -> L1N9 = 0.170350581407547\n",
            "L0N2                   -> L1N10 = 0.17016229033470154\n",
            "L0N2                   -> L1N11 = -0.044423263520002365\n",
            "L0N2                   -> L1N12 = 0.3844909965991974\n",
            "L0N2                   -> L1N13 = 0.13715419173240662\n",
            "L0N2                   -> L1N14 = -0.20663587749004364\n",
            "L0N2                   -> L1N15 = -0.14096608757972717\n",
            "L0N2                   -> L1N16 = 0.44271498918533325\n",
            "L0N2                   -> L1N17 = -0.3505961298942566\n",
            "L0N2                   -> L1N18 = 0.41298067569732666\n",
            "L0N2                   -> L1N19 = 0.3887811005115509\n",
            "L0N2                   -> L1N20 = -0.19172145426273346\n",
            "L0N2                   -> L1N21 = -0.2546614706516266\n",
            "L0N2                   -> L1N22 = 0.18840405344963074\n",
            "L0N2                   -> L1N23 = 0.4285317361354828\n",
            "L0N2                   -> L1N24 = 0.3399113118648529\n",
            "L0N3                   -> L1N0 = -0.003177778795361519\n",
            "L0N3                   -> L1N1 = -0.2161407768726349\n",
            "L0N3                   -> L1N2 = -0.32101568579673767\n",
            "L0N3                   -> L1N3 = -0.420497328042984\n",
            "L0N3                   -> L1N4 = -0.19540826976299286\n",
            "L0N3                   -> L1N5 = -0.322226345539093\n",
            "L0N3                   -> L1N6 = 0.24831688404083252\n",
            "L0N3                   -> L1N7 = 0.09865301102399826\n",
            "L0N3                   -> L1N8 = -0.1979135423898697\n",
            "L0N3                   -> L1N9 = -0.26423123478889465\n",
            "L0N3                   -> L1N10 = 0.11268838495016098\n",
            "L0N3                   -> L1N11 = 0.2772114872932434\n",
            "L0N3                   -> L1N12 = -0.26074475049972534\n",
            "L0N3                   -> L1N13 = -0.07647106051445007\n",
            "L0N3                   -> L1N14 = 0.3720940053462982\n",
            "L0N3                   -> L1N15 = -0.1993713676929474\n",
            "L0N3                   -> L1N16 = 0.35592684149742126\n",
            "L0N3                   -> L1N17 = -0.13057121634483337\n",
            "L0N3                   -> L1N18 = 0.3785100281238556\n",
            "L0N3                   -> L1N19 = -0.19364288449287415\n",
            "L0N3                   -> L1N20 = 0.4135226011276245\n",
            "L0N3                   -> L1N21 = 0.026546048000454903\n",
            "L0N3                   -> L1N22 = -0.28120166063308716\n",
            "L0N3                   -> L1N23 = 0.26417121291160583\n",
            "L0N3                   -> L1N24 = -0.4210524260997772\n",
            "L0N4                   -> L1N0 = -0.010112053714692593\n",
            "L0N4                   -> L1N1 = 0.37372782826423645\n",
            "L0N4                   -> L1N2 = -0.3295704126358032\n",
            "L0N4                   -> L1N3 = -0.08763113617897034\n",
            "L0N4                   -> L1N4 = -0.24112433195114136\n",
            "L0N4                   -> L1N5 = -0.04303157329559326\n",
            "L0N4                   -> L1N6 = 0.22631551325321198\n",
            "L0N4                   -> L1N7 = 0.7208731770515442\n",
            "L0N4                   -> L1N8 = 0.20004740357398987\n",
            "L0N4                   -> L1N9 = -0.38946378231048584\n",
            "L0N4                   -> L1N10 = 0.3969757854938507\n",
            "L0N4                   -> L1N11 = -0.1983610987663269\n",
            "L0N4                   -> L1N12 = 0.008315622806549072\n",
            "L0N4                   -> L1N13 = 0.08366039395332336\n",
            "L0N4                   -> L1N14 = -0.5093759894371033\n",
            "L0N4                   -> L1N15 = -0.26930785179138184\n",
            "L0N4                   -> L1N16 = -0.06869419664144516\n",
            "L0N4                   -> L1N17 = 0.17152675986289978\n",
            "L0N4                   -> L1N18 = 0.17985805869102478\n",
            "L0N4                   -> L1N19 = -0.0332951545715332\n",
            "L0N4                   -> L1N20 = 0.20794330537319183\n",
            "L0N4                   -> L1N21 = -0.06721407175064087\n",
            "L0N4                   -> L1N22 = 0.3994961678981781\n",
            "L0N4                   -> L1N23 = 0.451342910528183\n",
            "L0N4                   -> L1N24 = -0.34792742133140564\n",
            "L0N5                   -> L1N0 = -0.18169312179088593\n",
            "L0N5                   -> L1N1 = -0.13608485460281372\n",
            "L0N5                   -> L1N2 = 0.40067896246910095\n",
            "L0N5                   -> L1N3 = -0.23942111432552338\n",
            "L0N5                   -> L1N4 = -0.08387449383735657\n",
            "L0N5                   -> L1N5 = 0.08934435248374939\n",
            "L0N5                   -> L1N6 = 0.09460151940584183\n",
            "L0N5                   -> L1N7 = 0.44849979877471924\n",
            "L0N5                   -> L1N8 = -0.03540143370628357\n",
            "L0N5                   -> L1N9 = 0.1547931730747223\n",
            "L0N5                   -> L1N10 = 0.0902542993426323\n",
            "L0N5                   -> L1N11 = 0.2978653013706207\n",
            "L0N5                   -> L1N12 = 0.08172765374183655\n",
            "L0N5                   -> L1N13 = 0.2960980236530304\n",
            "L0N5                   -> L1N14 = 0.11889728158712387\n",
            "L0N5                   -> L1N15 = -0.2777513265609741\n",
            "L0N5                   -> L1N16 = 0.39485853910446167\n",
            "L0N5                   -> L1N17 = 0.22786113619804382\n",
            "L0N5                   -> L1N18 = -0.11305708438158035\n",
            "L0N5                   -> L1N19 = -0.34654897451400757\n",
            "L0N5                   -> L1N20 = 0.001998940482735634\n",
            "L0N5                   -> L1N21 = -0.33684319257736206\n",
            "L0N5                   -> L1N22 = -0.15080147981643677\n",
            "L0N5                   -> L1N23 = -0.31430691480636597\n",
            "L0N5                   -> L1N24 = -0.3113147020339966\n",
            "L0N6                   -> L1N0 = -0.19995461404323578\n",
            "L0N6                   -> L1N1 = 0.14389237761497498\n",
            "L0N6                   -> L1N2 = -0.12378835678100586\n",
            "L0N6                   -> L1N3 = 0.16936680674552917\n",
            "L0N6                   -> L1N4 = 0.2764216363430023\n",
            "L0N6                   -> L1N5 = -0.3515862822532654\n",
            "L0N6                   -> L1N6 = -0.5186717510223389\n",
            "L0N6                   -> L1N7 = 0.6583228707313538\n",
            "L0N6                   -> L1N8 = 0.27467891573905945\n",
            "L0N6                   -> L1N9 = 0.1688801348209381\n",
            "L0N6                   -> L1N10 = 0.3030186891555786\n",
            "L0N6                   -> L1N11 = 0.30813172459602356\n",
            "L0N6                   -> L1N12 = -0.08535224199295044\n",
            "L0N6                   -> L1N13 = 0.3737829625606537\n",
            "L0N6                   -> L1N14 = -0.17303122580051422\n",
            "L0N6                   -> L1N15 = -0.294653058052063\n",
            "L0N6                   -> L1N16 = 0.7686339020729065\n",
            "L0N6                   -> L1N17 = -0.25415289402008057\n",
            "L0N6                   -> L1N18 = 0.018651224672794342\n",
            "L0N6                   -> L1N19 = -0.21039195358753204\n",
            "L0N6                   -> L1N20 = 0.16654136776924133\n",
            "L0N6                   -> L1N21 = -0.31912916898727417\n",
            "L0N6                   -> L1N22 = 0.3464634120464325\n",
            "L0N6                   -> L1N23 = 0.5111557841300964\n",
            "L0N6                   -> L1N24 = -0.3858986794948578\n",
            "1B -> L2N0: -0.6762325167655945\n",
            "1B -> L2N1: 0.14627443253993988\n",
            "1B -> L2N2: 0.14340661466121674\n",
            "1B -> L2N3: 0.0\n",
            "1B -> L2N4: -0.13870513439178467\n",
            "1B -> L2N5: -0.13427312672138214\n",
            "1B -> L2N6: 0.0\n",
            "1B -> L2N7: 0.14335530996322632\n",
            "1B -> L2N8: -0.1389969140291214\n",
            "1B -> L2N9: 0.0\n",
            "L1N0                   -> L2N0 = 0.45370328426361084\n",
            "L1N0                   -> L2N1 = 0.21112875640392303\n",
            "L1N0                   -> L2N2 = -0.07751303166151047\n",
            "L1N0                   -> L2N3 = -0.31337279081344604\n",
            "L1N0                   -> L2N4 = 0.26979756355285645\n",
            "L1N0                   -> L2N5 = 0.035074394196271896\n",
            "L1N0                   -> L2N6 = -0.1202491819858551\n",
            "L1N0                   -> L2N7 = -0.20361566543579102\n",
            "L1N0                   -> L2N8 = -0.27341967821121216\n",
            "L1N0                   -> L2N9 = -0.3864523470401764\n",
            "L1N1                   -> L2N0 = -0.05081892013549805\n",
            "L1N1                   -> L2N1 = 0.027315139770507812\n",
            "L1N1                   -> L2N2 = 0.3490639626979828\n",
            "L1N1                   -> L2N3 = 0.030442625284194946\n",
            "L1N1                   -> L2N4 = -0.09650641679763794\n",
            "L1N1                   -> L2N5 = -0.14168515801429749\n",
            "L1N1                   -> L2N6 = 0.1318208873271942\n",
            "L1N1                   -> L2N7 = 0.29080888628959656\n",
            "L1N1                   -> L2N8 = -0.032969117164611816\n",
            "L1N1                   -> L2N9 = -0.2599128484725952\n",
            "L1N2                   -> L2N0 = 0.009311854839324951\n",
            "L1N2                   -> L2N1 = 0.19261649250984192\n",
            "L1N2                   -> L2N2 = 0.3505893051624298\n",
            "L1N2                   -> L2N3 = 0.37893542647361755\n",
            "L1N2                   -> L2N4 = -0.1312195360660553\n",
            "L1N2                   -> L2N5 = 0.07220059633255005\n",
            "L1N2                   -> L2N6 = 0.22206607460975647\n",
            "L1N2                   -> L2N7 = 0.27460744976997375\n",
            "L1N2                   -> L2N8 = -0.07488444447517395\n",
            "L1N2                   -> L2N9 = -0.39770787954330444\n",
            "L1N3                   -> L2N0 = -0.16760368645191193\n",
            "L1N3                   -> L2N1 = 0.3648979961872101\n",
            "L1N3                   -> L2N2 = -0.3057345449924469\n",
            "L1N3                   -> L2N3 = 0.040261656045913696\n",
            "L1N3                   -> L2N4 = 0.052255213260650635\n",
            "L1N3                   -> L2N5 = -0.11720195412635803\n",
            "L1N3                   -> L2N6 = 0.17981836199760437\n",
            "L1N3                   -> L2N7 = 0.22644761204719543\n",
            "L1N3                   -> L2N8 = 0.34556660056114197\n",
            "L1N3                   -> L2N9 = 0.05131584405899048\n",
            "L1N4                   -> L2N0 = 0.048432767391204834\n",
            "L1N4                   -> L2N1 = -0.16917069256305695\n",
            "L1N4                   -> L2N2 = 0.009347885847091675\n",
            "L1N4                   -> L2N3 = 0.30103543400764465\n",
            "L1N4                   -> L2N4 = -0.291948139667511\n",
            "L1N4                   -> L2N5 = 0.4043053090572357\n",
            "L1N4                   -> L2N6 = 0.25321224331855774\n",
            "L1N4                   -> L2N7 = -0.14960545301437378\n",
            "L1N4                   -> L2N8 = -0.05920630693435669\n",
            "L1N4                   -> L2N9 = -0.3935886323451996\n",
            "L1N5                   -> L2N0 = -0.05792832374572754\n",
            "L1N5                   -> L2N1 = 0.34722015261650085\n",
            "L1N5                   -> L2N2 = 0.13484057784080505\n",
            "L1N5                   -> L2N3 = -0.16300910711288452\n",
            "L1N5                   -> L2N4 = -0.020545780658721924\n",
            "L1N5                   -> L2N5 = -0.06749555468559265\n",
            "L1N5                   -> L2N6 = 0.2239464819431305\n",
            "L1N5                   -> L2N7 = 0.09340301156044006\n",
            "L1N5                   -> L2N8 = -0.20740683376789093\n",
            "L1N5                   -> L2N9 = 0.009332478046417236\n",
            "L1N6                   -> L2N0 = 0.05962656810879707\n",
            "L1N6                   -> L2N1 = -0.22714976966381073\n",
            "L1N6                   -> L2N2 = -0.21288591623306274\n",
            "L1N6                   -> L2N3 = 0.013993293046951294\n",
            "L1N6                   -> L2N4 = -0.22876904904842377\n",
            "L1N6                   -> L2N5 = 0.3738163709640503\n",
            "L1N6                   -> L2N6 = 0.08871588110923767\n",
            "L1N6                   -> L2N7 = 0.02498053014278412\n",
            "L1N6                   -> L2N8 = 0.18089509010314941\n",
            "L1N6                   -> L2N9 = 0.15053442120552063\n",
            "L1N7                   -> L2N0 = -0.20893989503383636\n",
            "L1N7                   -> L2N1 = -0.2709037661552429\n",
            "L1N7                   -> L2N2 = 0.28202298283576965\n",
            "L1N7                   -> L2N3 = -0.2318539172410965\n",
            "L1N7                   -> L2N4 = 0.37154048681259155\n",
            "L1N7                   -> L2N5 = -0.011869958601891994\n",
            "L1N7                   -> L2N6 = 0.3072528541088104\n",
            "L1N7                   -> L2N7 = -0.2067653238773346\n",
            "L1N7                   -> L2N8 = -0.3161640167236328\n",
            "L1N7                   -> L2N9 = 0.41092804074287415\n",
            "L1N8                   -> L2N0 = -0.312348335981369\n",
            "L1N8                   -> L2N1 = -0.30494245886802673\n",
            "L1N8                   -> L2N2 = 0.013933271169662476\n",
            "L1N8                   -> L2N3 = 0.0026271045207977295\n",
            "L1N8                   -> L2N4 = 0.2461826503276825\n",
            "L1N8                   -> L2N5 = 0.30583277344703674\n",
            "L1N8                   -> L2N6 = 0.08596667647361755\n",
            "L1N8                   -> L2N7 = -0.10601314902305603\n",
            "L1N8                   -> L2N8 = 0.25258490443229675\n",
            "L1N8                   -> L2N9 = 0.05062037706375122\n",
            "L1N9                   -> L2N0 = 0.3622331917285919\n",
            "L1N9                   -> L2N1 = -0.03811481595039368\n",
            "L1N9                   -> L2N2 = 0.007596403360366821\n",
            "L1N9                   -> L2N3 = 0.131196528673172\n",
            "L1N9                   -> L2N4 = 0.28730639815330505\n",
            "L1N9                   -> L2N5 = 0.31196513772010803\n",
            "L1N9                   -> L2N6 = 0.33535072207450867\n",
            "L1N9                   -> L2N7 = -0.34352362155914307\n",
            "L1N9                   -> L2N8 = 0.20816627144813538\n",
            "L1N9                   -> L2N9 = -0.3749488294124603\n",
            "L1N10                   -> L2N0 = -0.2006249725818634\n",
            "L1N10                   -> L2N1 = -0.3584844470024109\n",
            "L1N10                   -> L2N2 = -0.08526838570833206\n",
            "L1N10                   -> L2N3 = 0.3096071183681488\n",
            "L1N10                   -> L2N4 = -0.2852632403373718\n",
            "L1N10                   -> L2N5 = 0.18719540536403656\n",
            "L1N10                   -> L2N6 = -0.2965049147605896\n",
            "L1N10                   -> L2N7 = 0.41483432054519653\n",
            "L1N10                   -> L2N8 = 0.12993749976158142\n",
            "L1N10                   -> L2N9 = 0.26601442694664\n",
            "L1N11                   -> L2N0 = 0.09897029399871826\n",
            "L1N11                   -> L2N1 = 0.31462863087654114\n",
            "L1N11                   -> L2N2 = 0.13317972421646118\n",
            "L1N11                   -> L2N3 = 0.1670333445072174\n",
            "L1N11                   -> L2N4 = 0.014715004712343216\n",
            "L1N11                   -> L2N5 = 0.26492825150489807\n",
            "L1N11                   -> L2N6 = -0.03618919849395752\n",
            "L1N11                   -> L2N7 = 0.20741181075572968\n",
            "L1N11                   -> L2N8 = 0.0824626013636589\n",
            "L1N11                   -> L2N9 = 0.054770857095718384\n",
            "L1N12                   -> L2N0 = 0.1481289565563202\n",
            "L1N12                   -> L2N1 = 0.07786750793457031\n",
            "L1N12                   -> L2N2 = 0.12239417433738708\n",
            "L1N12                   -> L2N3 = -0.2102142721414566\n",
            "L1N12                   -> L2N4 = -0.18897709250450134\n",
            "L1N12                   -> L2N5 = 0.3265428841114044\n",
            "L1N12                   -> L2N6 = -0.23660583794116974\n",
            "L1N12                   -> L2N7 = -0.10281765460968018\n",
            "L1N12                   -> L2N8 = -0.3091975450515747\n",
            "L1N12                   -> L2N9 = -0.355803906917572\n",
            "L1N13                   -> L2N0 = -0.40562745928764343\n",
            "L1N13                   -> L2N1 = 0.29820653796195984\n",
            "L1N13                   -> L2N2 = 0.008969306945800781\n",
            "L1N13                   -> L2N3 = -0.005772650241851807\n",
            "L1N13                   -> L2N4 = -0.2160816788673401\n",
            "L1N13                   -> L2N5 = -0.27635377645492554\n",
            "L1N13                   -> L2N6 = 0.017299562692642212\n",
            "L1N13                   -> L2N7 = -0.27392345666885376\n",
            "L1N13                   -> L2N8 = -0.09040546417236328\n",
            "L1N13                   -> L2N9 = 0.33946308493614197\n",
            "L1N14                   -> L2N0 = -0.23231321573257446\n",
            "L1N14                   -> L2N1 = 0.37561026215553284\n",
            "L1N14                   -> L2N2 = 0.1258087009191513\n",
            "L1N14                   -> L2N3 = -0.2283059060573578\n",
            "L1N14                   -> L2N4 = 0.2517105042934418\n",
            "L1N14                   -> L2N5 = 0.3019482493400574\n",
            "L1N14                   -> L2N6 = -0.27514219284057617\n",
            "L1N14                   -> L2N7 = -0.30226826667785645\n",
            "L1N14                   -> L2N8 = 0.22572633624076843\n",
            "L1N14                   -> L2N9 = -0.1340728998184204\n",
            "L1N15                   -> L2N0 = -0.08921366930007935\n",
            "L1N15                   -> L2N1 = 0.21074292063713074\n",
            "L1N15                   -> L2N2 = -0.1896948516368866\n",
            "L1N15                   -> L2N3 = -0.3408574163913727\n",
            "L1N15                   -> L2N4 = -0.3664110004901886\n",
            "L1N15                   -> L2N5 = -0.024245798587799072\n",
            "L1N15                   -> L2N6 = 0.019084125757217407\n",
            "L1N15                   -> L2N7 = 0.409763902425766\n",
            "L1N15                   -> L2N8 = 0.29633650183677673\n",
            "L1N15                   -> L2N9 = 0.3254496157169342\n",
            "L1N16                   -> L2N0 = -0.1426752507686615\n",
            "L1N16                   -> L2N1 = 0.3885260820388794\n",
            "L1N16                   -> L2N2 = -0.3007014989852905\n",
            "L1N16                   -> L2N3 = 0.06520569324493408\n",
            "L1N16                   -> L2N4 = -0.07047051936388016\n",
            "L1N16                   -> L2N5 = -0.3049098253250122\n",
            "L1N16                   -> L2N6 = 0.05751451849937439\n",
            "L1N16                   -> L2N7 = -0.08967564254999161\n",
            "L1N16                   -> L2N8 = -0.019952574744820595\n",
            "L1N16                   -> L2N9 = 0.07858073711395264\n",
            "L1N17                   -> L2N0 = 0.2018129527568817\n",
            "L1N17                   -> L2N1 = 0.13499608635902405\n",
            "L1N17                   -> L2N2 = 0.017749279737472534\n",
            "L1N17                   -> L2N3 = 0.03069344162940979\n",
            "L1N17                   -> L2N4 = -0.14328187704086304\n",
            "L1N17                   -> L2N5 = -0.34498143196105957\n",
            "L1N17                   -> L2N6 = -0.40919333696365356\n",
            "L1N17                   -> L2N7 = 0.33311256766319275\n",
            "L1N17                   -> L2N8 = 0.29885128140449524\n",
            "L1N17                   -> L2N9 = 0.10627266764640808\n",
            "L1N18                   -> L2N0 = 0.20961083471775055\n",
            "L1N18                   -> L2N1 = -0.3741220235824585\n",
            "L1N18                   -> L2N2 = 0.0908036082983017\n",
            "L1N18                   -> L2N3 = 0.0741705596446991\n",
            "L1N18                   -> L2N4 = 0.057225022464990616\n",
            "L1N18                   -> L2N5 = -0.2577798068523407\n",
            "L1N18                   -> L2N6 = -0.21435387432575226\n",
            "L1N18                   -> L2N7 = 0.32694995403289795\n",
            "L1N18                   -> L2N8 = 0.10509737581014633\n",
            "L1N18                   -> L2N9 = -0.26831772923469543\n",
            "L1N19                   -> L2N0 = -0.003128260374069214\n",
            "L1N19                   -> L2N1 = 0.2170616090297699\n",
            "L1N19                   -> L2N2 = 0.14230862259864807\n",
            "L1N19                   -> L2N3 = -0.016972512006759644\n",
            "L1N19                   -> L2N4 = -0.13472658395767212\n",
            "L1N19                   -> L2N5 = 0.37053439021110535\n",
            "L1N19                   -> L2N6 = -0.1699424386024475\n",
            "L1N19                   -> L2N7 = -0.1026490330696106\n",
            "L1N19                   -> L2N8 = -0.027602314949035645\n",
            "L1N19                   -> L2N9 = -0.12801319360733032\n",
            "L1N20                   -> L2N0 = -0.008238577283918858\n",
            "L1N20                   -> L2N1 = 0.02324254997074604\n",
            "L1N20                   -> L2N2 = 0.18784765899181366\n",
            "L1N20                   -> L2N3 = 0.08952662348747253\n",
            "L1N20                   -> L2N4 = -0.10268992930650711\n",
            "L1N20                   -> L2N5 = 0.11728127300739288\n",
            "L1N20                   -> L2N6 = -0.2316039651632309\n",
            "L1N20                   -> L2N7 = 0.4167625308036804\n",
            "L1N20                   -> L2N8 = 0.2311270534992218\n",
            "L1N20                   -> L2N9 = -0.3322676718235016\n",
            "L1N21                   -> L2N0 = 0.6341477036476135\n",
            "L1N21                   -> L2N1 = 0.03553713113069534\n",
            "L1N21                   -> L2N2 = -0.0703340396285057\n",
            "L1N21                   -> L2N3 = 0.23625430464744568\n",
            "L1N21                   -> L2N4 = 0.09821494668722153\n",
            "L1N21                   -> L2N5 = 0.09340850263834\n",
            "L1N21                   -> L2N6 = 0.3968139588832855\n",
            "L1N21                   -> L2N7 = 0.02294394001364708\n",
            "L1N21                   -> L2N8 = -0.1559610366821289\n",
            "L1N21                   -> L2N9 = 0.1078425943851471\n",
            "L1N22                   -> L2N0 = -0.34967559576034546\n",
            "L1N22                   -> L2N1 = 0.25662270188331604\n",
            "L1N22                   -> L2N2 = -0.2680954337120056\n",
            "L1N22                   -> L2N3 = 0.0517442524433136\n",
            "L1N22                   -> L2N4 = 0.11940094828605652\n",
            "L1N22                   -> L2N5 = 0.41024288535118103\n",
            "L1N22                   -> L2N6 = 0.2362610399723053\n",
            "L1N22                   -> L2N7 = 0.18315014243125916\n",
            "L1N22                   -> L2N8 = 0.02520766854286194\n",
            "L1N22                   -> L2N9 = 0.09398671984672546\n",
            "L1N23                   -> L2N0 = 0.19227328896522522\n",
            "L1N23                   -> L2N1 = 0.049861498177051544\n",
            "L1N23                   -> L2N2 = 0.4136495590209961\n",
            "L1N23                   -> L2N3 = -0.28334102034568787\n",
            "L1N23                   -> L2N4 = 0.14351357519626617\n",
            "L1N23                   -> L2N5 = -0.38325339555740356\n",
            "L1N23                   -> L2N6 = -0.3542456030845642\n",
            "L1N23                   -> L2N7 = 0.13270270824432373\n",
            "L1N23                   -> L2N8 = 0.09847385436296463\n",
            "L1N23                   -> L2N9 = -0.3876612186431885\n",
            "L1N24                   -> L2N0 = 0.20973995327949524\n",
            "L1N24                   -> L2N1 = 0.19763633608818054\n",
            "L1N24                   -> L2N2 = 0.3829384744167328\n",
            "L1N24                   -> L2N3 = -0.4035262167453766\n",
            "L1N24                   -> L2N4 = 0.0033635199069976807\n",
            "L1N24                   -> L2N5 = 0.11149606108665466\n",
            "L1N24                   -> L2N6 = -0.24292190372943878\n",
            "L1N24                   -> L2N7 = -0.3137739598751068\n",
            "L1N24                   -> L2N8 = -0.2512809634208679\n",
            "L1N24                   -> L2N9 = -0.13416004180908203\n",
            "2B -> L3N0: 0.14128221571445465\n",
            "L2N0                   -> L3N0 = -0.40145131945610046\n",
            "L2N1                   -> L3N0 = 0.22793881595134735\n",
            "L2N2                   -> L3N0 = 0.24221795797348022\n",
            "L2N3                   -> L3N0 = 0.27456802129745483\n",
            "L2N4                   -> L3N0 = -0.1295851618051529\n",
            "L2N5                   -> L3N0 = -0.32738566398620605\n",
            "L2N6                   -> L3N0 = 0.12758928537368774\n",
            "L2N7                   -> L3N0 = 0.3968394696712494\n",
            "L2N8                   -> L3N0 = -0.5147376656532288\n",
            "L2N9                   -> L3N0 = -0.4472431242465973\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZ-VstJqj5wt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "d75da000-bd41-4f4a-adb2-c91410d49230"
      },
      "source": [
        "input0 = 0\n",
        "input1 = 1\n",
        "\n",
        "hidden0Sum = (input0*1.3)+(input1*1.3)+(-1.3)\n",
        "hidden1Sum = (input0*1.2)+(input1*1.2)+(0)\n",
        "\n",
        "print(hidden0Sum) # 0\n",
        "print(hidden1Sum) # 1.2\n",
        "\n",
        "hidden0 = max(0,hidden0Sum)\n",
        "hidden1 = max(0,hidden1Sum)\n",
        "\n",
        "print(hidden0) # 0\n",
        "print(hidden1) # 1.2\n",
        "\n",
        "outputSum = (hidden0*-1.6)+(hidden1*0.8)+(0)\n",
        "print(outputSum) # 0.96\n",
        "\n",
        "output = max(0,outputSum)\n",
        "\n",
        "print(output) # 0.96"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0\n",
            "1.2\n",
            "0\n",
            "1.2\n",
            "0.96\n",
            "0.96\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIHi6w9lj9dO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}